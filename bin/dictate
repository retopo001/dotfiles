#!/home/bw/whisper-live/bin/python
"""
Fast real-time dictation using faster-whisper.
Toggle: run once to start, run again to stop.
"""
import os
import sys
import signal
import subprocess
import numpy as np
import sounddevice as sd
from pathlib import Path

PID_FILE = "/tmp/whisper-dictate.pid"
MODEL_ID = "deepdml/faster-whisper-large-v3-turbo-ct2"
SAMPLE_RATE = 16000
CHUNK_MS = 500  # Process every 500ms
CHUNK_SAMPLES = int(SAMPLE_RATE * CHUNK_MS / 1000)
DEVICE = 13  # pipewire (routes to MOTU M2)

# Hallucination filter
HALLUCINATIONS = {
    "thank you", "thanks for watching", "subscribe", "bye",
    "okay", "you", "the", "so", "um", "uh", ""
}

def notify(msg):
    subprocess.run(["notify-send", "-t", "1000", msg], check=False)

def type_text(text):
    subprocess.run(["xdotool", "type", "--delay", "3", "--", text], check=False)

def is_hallucination(text):
    t = text.strip().lower()
    if len(t) < 2:
        return True
    if t in HALLUCINATIONS:
        return True
    if all(c in ".,!?;:-" for c in t):
        return True
    return False

def toggle_check():
    """If already running, kill it and exit."""
    if Path(PID_FILE).exists():
        try:
            pid = int(Path(PID_FILE).read_text().strip())
            os.kill(pid, signal.SIGTERM)
        except (ProcessLookupError, ValueError):
            pass
        Path(PID_FILE).unlink(missing_ok=True)
        notify("Dictation OFF")
        sys.exit(0)

def cleanup(signum=None, frame=None):
    Path(PID_FILE).unlink(missing_ok=True)
    sys.exit(0)

def main():
    toggle_check()

    # Write PID
    Path(PID_FILE).write_text(str(os.getpid()))
    signal.signal(signal.SIGTERM, cleanup)
    signal.signal(signal.SIGINT, cleanup)

    notify("Dictation ON")

    # Load model
    from faster_whisper import WhisperModel
    model = WhisperModel(MODEL_ID, device="cuda", compute_type="float16")

    # Audio buffer
    buffer = np.zeros(SAMPLE_RATE * 4, dtype=np.float32)  # 4 sec buffer
    write_idx = 0

    def audio_callback(indata, frames, time, status):
        nonlocal buffer, write_idx
        mono = indata[:, 0] if indata.ndim > 1 else indata.flatten()
        n = len(mono)
        # Circular buffer write
        end_idx = write_idx + n
        if end_idx <= len(buffer):
            buffer[write_idx:end_idx] = mono
        else:
            first = len(buffer) - write_idx
            buffer[write_idx:] = mono[:first]
            buffer[:n - first] = mono[first:]
        write_idx = end_idx % len(buffer)

    # Start audio stream
    stream = sd.InputStream(
        samplerate=SAMPLE_RATE,
        channels=1,
        dtype=np.float32,
        device=DEVICE,
        callback=audio_callback,
        blocksize=CHUNK_SAMPLES
    )

    import time
    last_text = ""

    with stream:
        while True:
            time.sleep(CHUNK_MS / 1000)

            # Get last 3 seconds of audio
            audio = np.roll(buffer, -write_idx)[-SAMPLE_RATE * 3:]

            # Simple VAD: skip if too quiet
            rms = np.sqrt(np.mean(audio ** 2))
            if rms < 0.005:
                continue

            # Transcribe
            segments, _ = model.transcribe(
                audio,
                language="en",
                beam_size=1,
                vad_filter=True,
                vad_parameters=dict(min_silence_duration_ms=300),
            )

            text = "".join(s.text for s in segments).strip()

            # Skip if same as last or hallucination
            if text == last_text or is_hallucination(text):
                continue

            # Find new portion
            if text.startswith(last_text):
                new_text = text[len(last_text):].strip()
            else:
                new_text = text

            if new_text and not is_hallucination(new_text):
                type_text(new_text + " ")
                last_text = text

if __name__ == "__main__":
    main()
